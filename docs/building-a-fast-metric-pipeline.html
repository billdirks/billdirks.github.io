<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://billdirks.com/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://billdirks.com/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="bill dirks" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="c++, performance, metrics, metrics, " />

<meta property="og:title" content="Building a fast metric pipeline "/>
<meta property="og:url" content="https://billdirks.com/building-a-fast-metric-pipeline.html" />
<meta property="og:description" content="Building a proof of concept for a fast metric pipeline" />
<meta property="og:site_name" content="bill&#39;s mumbles" />
<meta property="og:article:author" content="bill dirks" />
<meta property="og:article:published_time" content="2023-05-21T00:00:00-07:00" />
<meta name="twitter:title" content="Building a fast metric pipeline ">
<meta name="twitter:description" content="Building a proof of concept for a fast metric pipeline">

        <title>Building a fast metric pipeline  · bill&#39;s mumbles
</title>
        <link href="https://billdirks.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="bill&#39;s mumbles - Full Atom Feed" />



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://billdirks.com/"><span class=site-name>bill's mumbles</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://billdirks.com
                                    >Home</a>
                                </li>
                                <li ><a href="https://billdirks.com/pages/about.html">About</a></li>
                                <li ><a href="https://billdirks.com/categories.html">Categories</a></li>
                                <li ><a href="https://billdirks.com/tags.html">Tags</a></li>
                                <li ><a href="https://billdirks.com/archives.html">Archives</a></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://billdirks.com/building-a-fast-metric-pipeline.html">
                Building a fast metric&nbsp;pipeline
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h1>Motivation</h1>
<p>Lately, I&#8217;ve been frustrated with slow software.  Poor startup times, poor response times, it feels like a slog to use a lot of&nbsp;tools.</p>
<p>My path to writing software started with analyzing biological data and a lot of my interest in computing revolves around processing and getting insights from&nbsp;data.</p>
<p>Lately I&#8217;ve been writing a lot of Python. While it&#8217;s great to build functional things quickly, a downside is, it is&nbsp;slow.</p>
<h1>Goals</h1>
<p>For this project I wanted to build a metric pipeline meeting the following&nbsp;criteria:</p>
<ul>
<li>I want to compute metrics over numeric&nbsp;data.</li>
<li>I want the computation to be fast. That is, how long would it take to compute metrics on 1 billion data points on my&nbsp;laptop?</li>
<li>The pipeline is allowed to see each data point exactly once.<ul>
<li>This is so we can use the pipeline for streaming data or reading&nbsp;files.</li>
<li>It encourages writing O(N)&nbsp;metrics</li>
</ul>
</li>
<li>Metrics are defined outside of the core pipeline for easy&nbsp;extension.</li>
</ul>
<p>Since metrics are defined outside of the main pipeline, I have some metric specific&nbsp;criteria:</p>
<ul>
<li>For metrics, I&#8217;m willing to sacrifice exactness if I can bound the&nbsp;error.</li>
<li>The metrics should have bounded memory, independent of the data size. That is, the metric can&#8217;t just store all the data to get around the seeing data once&nbsp;criteria.</li>
<li>Metrics should be summable. That is, if the same metric is computed over 2 data sets, we can combine the results to get an aggregate&nbsp;result.</li>
</ul>
<h1>Metrics</h1>
<p>I find estimating a distribution will answer most questions about one dimensional numeric data. In addition I&#8217;d like to compute some descriptive statistics. The metrics I am interested in&nbsp;are:</p>
<ul>
<li>min</li>
<li>max</li>
<li>mean</li>
<li>a distribution&nbsp;estimate</li>
</ul>
<p>To estimate the distribution I&#8217;ve chosen to use a very slight variation of <a href="https://arxiv.org/pdf/1908.10693.pdf">DDSketch</a>. DDSketch is basically an algorithm for generating a histogram. It is a bit more complicated than a traditional histogram with fixed size bins because it scales the bin size to bound the relative error. That means we will likely have less bins overall and we can still make meaningful statements about percentiles. For example, we could use this distribution and infer that our estimate of the median is within 0.01% of the true empirical&nbsp;median.</p>
<h1>Pipeline&nbsp;architecture</h1>
<p>I&#8217;d like to describe the architecture and some high level implementation details. The code is all in C++ and is organized into 2 logical components: the pipeline and the metrics. All the code is available <a href="https://github.com/billdirks/billdirks.github.io/tree/publish/code/metric_playground">here</a>.</p>
<p>The metric interface lives in <code>metric.hpp</code> which defines a <code>Metric</code> template class. The important parts of the interface&nbsp;are:</p>
<ul>
<li>A <code>process</code> method which takes a batch of input data and is responsible for updating the metric&#8217;s state using this&nbsp;data.</li>
<li>A <code>value</code> method which returns the current value of the metric. It should be callable and return an accurate value after any arbitrary number of calls to <code>process</code>.</li>
</ul>
<p>I have not yet implemented an <code>add</code> method to allow combining of metrics though all the current metrics could support this type of operation. An <code>add</code> method would allow one to distribute computations over many nodes and then combine them&nbsp;afterwards.</p>
<p>While one could envision different uses for a metrics pipeline, such as processing streaming data or batch processing, for this project I have written a pipeline that reads a data file from disk, looping over the data and calling the metrics <code>process</code> method on each data point. The pipeline is implemented in <code>compute_metrics.cpp</code> and we setup and run it in <code>main.cpp</code>.</p>
<h1>Learnings</h1>
<p>It took me a few passes before I settled on my final code optimizations. In this section I enumerate the lessons learned and the things that mattered to make progress. There were 2 classes of optimization: <span class="caps">IO</span> (reading the data) and&nbsp;compute.</p>
<h2>Compute</h2>
<p>For optimizing compute, I focus on DDSketch. The other metrics were more straightforward, and while they likely can be more optimized, most of the compute time was spent in DDSketch. Since DDSketch basically produces a histogram, difficulties arise from storing and sorting the histogram as new data&nbsp;arrives.</p>
<p>There are 3 main lessons about&nbsp;compute:</p>
<ol>
<li>Choosing the correct data structure is&nbsp;important.</li>
<li>Asymptotics may matter but constants also&nbsp;matter.</li>
<li>Measuring where time is spent is important and can be&nbsp;surprising.</li>
</ol>
<p>Here are the high level steps we do when creating a histogram. The <code>process</code> method looks at each point in a batch, determines the bucket it falls into, and increments the counter on that bucket by one. The edges of the bucket are determined by DDSketch which is parameterized by the relative error we allow, in our case 0.01%. A call to <code>value</code> will produce the current&nbsp;histogram.</p>
<p>There are a number of data structures that can store histogram data. I tried 3: <code>map</code>, <code>unordered_map</code>, and <code>vector</code> which are all available in the C++ standard&nbsp;library.</p>
<p><code>map</code>: This is a dictionary with an ordering on the keys. So every <a href="https://en.cppreference.com/w/cpp/container/map">insert is <code>log(n)</code></a> where <code>n</code> is the current size of the <code>map</code>. If we insert <code>N</code> data points the number of operations is <code>log(1) + log(2) + ... + log(N)</code> which is <code>O(N log(N))</code>. For my initial implementation, I computed the bucket for each data point which I used as a key into a <code>map</code> and incremented the key&#8217;s value by 1. This was disappointingly&nbsp;slow.</p>
<p><code>unordered_map</code>: An <code>unordered_map</code> is like a <code>map</code>, however, the keys are unordered. Since a histogram is ordered data, I end up ordering the <code>keys</code> on a call to <code>value</code>. In my code, I only call <code>value</code> once, after I see all the data from the input file. Inserts into an <code>unordered_map</code> are <a href="https://en.cppreference.com/w/cpp/container/unordered_map">O(1) on average</a>. However, we need to sort the <code>unordered_map</code> before outputting a histogram so the asymptotic time is once again <code>O(N log(N))</code>. The reason I chose to try <code>map</code> first was because the asymptotic times are the same but <code>map</code> was easier to implement since I didn&#8217;t have to think about sorting. <code>unordered_map</code> proved to be much faster, around 4X for my&nbsp;data.</p>
<p>This still proved to be slow and I hoped that I could do better so I profiled the code. I compiled my code using <code>gperftools</code>, ran it on a test file with 100,000,000 points, and looked at the output with <code>gprof</code>. The results were surprising. The time wasn&#8217;t spent computing the bucket for a data point (my code) but instead was spent inserting numbers into the <code>unordered_map</code> (stdlib code). That means, computing the hash for the bucket, was the most expensive operation I was&nbsp;doing.</p>
<p>I hadn&#8217;t thought too hard about implementing a bound on memory usage at this point. However, now that I understood how expensive hashing was, I wanted to predetermine the amount of memory needed and store the histogram in a list. The bucket produced by DDSketch is an integer representing the first bucket, second bucket, etc, so can be used as indexes into a&nbsp;list.</p>
<p><code>vector</code>: A <code>vector</code> in C++ is a dynamically resizable list that is stored in a contiguous section of memory. In order to use it, I wanted to determine the amount of memory necessary up front so I could do 1 allocation and avoid copying and moving data around. DDSketch&#8217;s strategy to bound memory usage is to predetermine the number of buckets allowed and to combine the smallest index buckets when the maximum number of buckets is reached. I&#8217;m guessing this is because the authors are interested in accurately estimating the highest value percentiles (eg the p99 response time for web requests for an app). My use case is more inspired by data quality so I follow <a href="https://greatexpectations.io/">Great Expectations</a> lead and presume the user has some idea of the range the data should lie in. So this implementation requires the user to provide a lower and upper bound for the data. I then use these bounds to determine the number of buckets. I also add 2 additional buckets that are used when the data falls below or above this range. Using this strategy, I can preallocate the needed memory. The <code>vector</code> data structure also lets me avoid sorting and, since inserts are O(1), this results in an O(N) algorithm. So, while the major speedup is due to eliminating hashing, rethinking the algorithm also resulted in a faster asymptotic&nbsp;runtime!</p>
<h2><span class="caps">IO</span></h2>
<p>I like computing things and I thought the interesting part of this project would be computing metrics.  However, that wasn&#8217;t right. The actual performance bottleneck was reading the data from disk, ie <span class="caps">IO</span>.</p>
<p>Since I didn&#8217;t initially think about <span class="caps">IO</span> at all, I implemented reading data in the most naive way possible.  I opened a file and read in the data one point at a time like&nbsp;this:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># variables</span>
<span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ifstream</span><span class="w"> </span><span class="p">{</span><span class="n">filename</span><span class="p">,</span><span class="w"> </span><span class="n">ios</span><span class="p">::</span><span class="ow">in</span><span class="p">};</span>
<span class="n">double</span><span class="w"> </span><span class="n">value</span><span class="p">;</span>

<span class="c1"># read the data in a loop using</span>
<span class="n">input</span><span class="w"> </span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">value</span><span class="p">;</span>
</code></pre></div>

<p>I initially picked a batch size, read in at most batch size points, and updated the metrics before starting the next cycle. This was horrendously slow primarily because I was doing a separate read call for each data&nbsp;point.</p>
<p>After some searching I came across this <a href="https://lemire.me/blog/2012/06/26/which-is-fastest-read-fread-ifstream-or-mmap/">fantastic blog post</a> about reading data which had some <a href="https://github.com/lemire/Code-used-on-Daniel-Lemire-s-blog/blob/master/2012/06/26/ioaccess.cpp">sample benchmarking code</a>. Running the benchmarks on my machine indicated that <code>mmap</code> was going to be the fastest. I did try using <code>ifstream</code> to read in the whole file at once but <code>mmap</code> was significantly&nbsp;faster.</p>
<h1>Setup for&nbsp;Timings</h1>
<p>All the timings were done on my laptop which is a 1st gen M1 powerbook with 16 <span class="caps">GB</span> of <span class="caps">RAM</span>. They  were done by modifying the <code>compute_metrics</code> function in <code>compute_metrics.cpp</code>. The function is defined&nbsp;as:</p>
<div class="highlight"><pre><span></span><code><span class="nb nb-Type">void</span><span class="w"> </span><span class="n">compute_metrics</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">MmapFile</span><span class="w"> </span><span class="n">mmap_file</span><span class="p">,</span><span class="w"> </span><span class="n">ComputeMetricOutput</span><span class="o">*</span><span class="w"> </span><span class="n">computed_metrics</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nb">char</span><span class="o">*</span><span class="w"> </span><span class="n">cur_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mmap_file</span><span class="o">.</span><span class="n">start</span><span class="p">();</span>
<span class="w">    </span><span class="nb">char</span><span class="o">*</span><span class="w"> </span><span class="n">end_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mmap_file</span><span class="o">.</span><span class="n">end</span><span class="p">();</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">cur_ptr</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">end_ptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="o">&lt;</span><span class="n">CODE</span><span class="o">&gt;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p>Where <code>&lt;CODE&gt;</code> is one of three code&nbsp;snippets:</p>
<p><strong>Reading Data&nbsp;Only</strong></p>
<div class="highlight"><pre><span></span><code>strtod(cur_ptr, &amp;cur_ptr);
</code></pre></div>

<p><strong>Compute Histogram&nbsp;Only</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">double</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">strtod</span><span class="p">(</span><span class="n">cur_ptr</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">cur_ptr</span><span class="p">);</span>
<span class="n">computed_metrics</span><span class="o">-&gt;</span><span class="n">ddsketch</span><span class="p">.</span><span class="nf">process</span><span class="p">(</span><span class="n">d</span><span class="p">);</span>
</code></pre></div>

<p><strong>Compute Histogram and Descriptive&nbsp;Metrics</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">double</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">strtod</span><span class="p">(</span><span class="n">cur_ptr</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">cur_ptr</span><span class="p">);</span>
<span class="n">computed_metrics</span><span class="o">-&gt;</span><span class="n">ddsketch</span><span class="p">.</span><span class="n">process</span><span class="p">(</span><span class="n">d</span><span class="p">);</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">computed_metrics</span><span class="o">-&gt;</span><span class="n">metrics</span><span class="p">.</span><span class="k">size</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">    </span><span class="n">computed_metrics</span><span class="o">-&gt;</span><span class="n">metrics</span><span class="o">[</span><span class="n">i</span><span class="o">]-&gt;</span><span class="n">process</span><span class="p">(</span><span class="n">d</span><span class="p">);</span>
<span class="err">}</span>
</code></pre></div>

<p>The code was built and run using the&nbsp;command:</p>
<div class="highlight"><pre><span></span><code>make clean
make compute_opt
rm o
gtime -v build/compute data/&lt;datafile&gt; &gt; o
</code></pre></div>

<p>I use the compiler flag <code>-O3</code>. This optimizes for speed (as opposed to binary size) while keeping the computations accurate. That is, one can get faster binaries if one is willing to do inaccurate math, but this is antithetical to what I am trying to&nbsp;do.</p>
<p>The data was generated using the code in <code>random_number.cpp</code> which can be compiled as run as&nbsp;follows:</p>
<div class="highlight"><pre><span></span><code>g++ -Wall --std=c++20 -Iinclude -O3 utils/random_number.cpp -o random_number
./random_number
</code></pre></div>

<p>Running it produces a 100 million normally distributed ascii encoded&nbsp;numbers.</p>
<h3>Timings</h3>
<p><code>gtime -v</code> prints out some nice diagnostic information about a pipeline run. To determine speed I report the <code>Elapsed (wall clock) time</code>. I did timings with 2 data sets, one with 100 million data points and one with 1 billion data points. If I run the pipeline more than once in succession subsequent runs are significantly faster since the <span class="caps">OS</span> caches data. To normalize for this I reboot between timings. I report timing both the &#8220;cold&#8221; and &#8220;hot&#8221;&nbsp;runs.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Number of data points</th>
<th>Cold Speed</th>
<th>Hot Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td>Reading Data Only</td>
<td>100 million</td>
<td>3.69 s</td>
<td>1.89 s</td>
</tr>
<tr>
<td>Compute Histogram Only</td>
<td>100 million</td>
<td>4.21 s</td>
<td>2.45 s</td>
</tr>
<tr>
<td>Compute Histogram and Descriptive Metrics</td>
<td>100 million</td>
<td>4.59 s</td>
<td>2.81 s</td>
</tr>
<tr>
<td>Reading Data Only</td>
<td>1 billion</td>
<td>32.40 s</td>
<td>17.31 s</td>
</tr>
<tr>
<td>Compute Histogram Only</td>
<td>1 billion</td>
<td>39.14 s</td>
<td>23.24 s</td>
</tr>
<tr>
<td>Compute Histogram and Descriptive Metrics</td>
<td>1 billion</td>
<td>41.84 s</td>
<td>26.19 s</td>
</tr>
</tbody>
</table>
<p>From these measurements we see that, despite focusing on what I though was a computational problem, reading in the data dominates the timings. While I would need to do more timings, the cold runs suggest a possible linear scaling. If I don&#8217;t reboot between runs and have other apps running (eg browser, editor) the 100 million data point runs are very similar to what I report here while the 1 billion data point runs are seconds slower and the &#8220;hot&#8221; runs take the same amount of time as the &#8220;cold&#8221;&nbsp;runs.</p>
<p>I also examined the memory output of <code>gtime -v</code>.  Both data sets fit completely into memory and, for a particular data set, the same amount of memory is used across&nbsp;runs.</p>
<table>
<thead>
<tr>
<th>Number of data points</th>
<th>File size</th>
<th>Max resident memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>100 million</td>
<td>744 <span class="caps">MB</span></td>
<td>745 <span class="caps">MB</span></td>
</tr>
<tr>
<td>1 billion</td>
<td>6753 <span class="caps">MB</span></td>
<td>6754 <span class="caps">MB</span></td>
</tr>
</tbody>
</table>
<h1>Comparison to&nbsp;Python</h1>
<p>Since I spend a lot of time looking at Python code, I was curious how fast a similar computation would be there. For this quick comparison, I only looked at computing a histogram using numpy. This would be comparable to the &#8220;Compute Histogram Only&#8221; numbers, however, the Python histogram is a little simpler to compute than DDSketch. I also don&#8217;t write the histogram data to a file. This gives the Python code a slight advantage. The script I used&nbsp;was:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;/path/to/data.csv&quot;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">hist</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="kp">histogram</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">80591</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10000000</span><span class="p">))</span>
</code></pre></div>

<table>
<thead>
<tr>
<th>Language</th>
<th>Number of data points</th>
<th>Speed</th>
<th>Max resident memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>Python</td>
<td>100 million</td>
<td>5.84 s</td>
<td>2262 <span class="caps">MB</span></td>
</tr>
<tr>
<td>C++</td>
<td>100 million</td>
<td>4.21 s</td>
<td>745 <span class="caps">MB</span></td>
</tr>
<tr>
<td>Python</td>
<td>1 billion</td>
<td>48.20 s</td>
<td>8773 <span class="caps">MB</span></td>
</tr>
<tr>
<td>C++</td>
<td>1 billion</td>
<td>39.14 s</td>
<td>6754 <span class="caps">MB</span></td>
</tr>
</tbody>
</table>
<p>The Python code is significantly slower and consumes more memory but is much, much simpler. Most of the time in Python was also spent in <span class="caps">IO</span> (~4.8 s and ~40.3&nbsp;s).</p>
<h1>Final&nbsp;Thoughts</h1>
<p>This project was a lot of fun! The two things I found surprising, but are &#8220;obvious&#8221; in&nbsp;retrospect:</p>
<ol>
<li>I thought compute would dominate the timings since this project was about summarizing data but <span class="caps">IO</span> dominates. The reason is, I&#8217;m not doing very many operations on any data point and the most complicated operation I&#8217;m doing is taking a&nbsp;log.</li>
<li>Using a map was slow since computing hashes is more expensive than computing the index (bucket id) into a vector. Having written mostly Python recently where dictionaries are a very common data structure it was good to be reminded of their&nbsp;cost.</li>
</ol>
<p>I also wanted to highlight 2 lessons that I took&nbsp;away:</p>
<ol>
<li>Experimenting by benchmarking is very helpful. I was lucky that I came across Daniel Lemire&#8217;s blog and could use his code to do timings of different <span class="caps">IO</span>&nbsp;strategies.</li>
<li>Profilings code is helpful and often surprising. I was very focused on the map data structures and it took the nudge from profiling to make me think of a different strategy&nbsp;altogether.</li>
</ol>
<p>Even though my code has a lot of shortcomings (eg I assume the data is well formed and only contains 1 column), it did teach me about data processing bottlenecks. I see why people develop new data formats and why so much attention is spent on <span class="caps">IO</span>.</p>
<p>Thanks for&nbsp;reading!</p>


             
 
            
            
            







            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2023-05-21T00:00:00-07:00">Sun 21 May 2023</time>
            <h4>Category</h4>
            <a class="category-link" href="https://billdirks.com/categories.html#metrics-ref">metrics</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://billdirks.com/tags.html#c-ref">c++
                    <span class="superscript">1</span>
</a></li>
                <li><a href="https://billdirks.com/tags.html#metrics-ref">metrics
                    <span class="superscript">1</span>
</a></li>
                <li><a href="https://billdirks.com/tags.html#performance-ref">performance
                    <span class="superscript">2</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://www.linkedin.com/in/bdirks/" title="LinkedIn" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="LinkedIn" role="img" viewBox="0 0 512 512" fill="#fff"><rect width="512" height="512" rx="15%" fill="#0077b5"/><circle cx="142" cy="138" r="37"/><path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://billdirks.com/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>